[{"role": "user", "content": "give me a summary of the document in detail "}, {"role": "assistant", "content": "The document discusses the development of a Real-Time Engagement Tracker, a browser-based application designed to help presenters, sales professionals, and business leaders measure and understand audience engagement in a concrete way. The application captures camera and screen activity during live presentations, analyzes viewer expressions every 10 seconds using an AI model, and assigns an engagement score from 1-5. The goal is to display results in real-time via an interactive app, providing a post-session review with a timeline of engagement scores and clickable timestamps linking to camera and screen snapshots.\n\nThe document highlights the importance of understanding audience engagement, which can be challenging to gauge, especially in presentations where the level of engagement can differ significantly. To address this, the developers utilized two primary datasets: the FER-2013 Dataset and the 135-class Emotional Facial Expression Dataset. These datasets were used to train machine learning models to detect engagement, but the initial results were not satisfactory due to the complexity of the emotion categories and the lack of consistency in the labels.\n\nTo improve the accuracy of the engagement detection, the developers performed a manual validation process, reviewing over 20,000 images and reassigning their engagement labels based on the visual context relevant to presentations. This process allowed for the creation of a clean and consistent dataset, with labels accurately reflecting engagement states. The developers also grouped the emotions into three broader categories: Engaged-Positive, Engaged-Negative, and Not Engaged, to make the output easier to interpret.\n\nThe document also discusses the technical aspects of the project, including the use of the ResNet-18 model, which did not perform well on the dataset, and the exploration of other model architectures. The developers acknowledge the challenges they faced, including the imbalance in the label distribution and the need for a more robust infrastructure.\n\nIn addition to the technical aspects, the document highlights the value proposition of the Real-Time Engagement Tracker, which is built on core values such as trust, learning, clarity, and actionability. The application aims to help presenters evolve and refine their delivery over time, making every presentation an opportunity to learn and improve.\n\nThe document also provides an overview of the market trends and the potential applications of the technology. The global emotion-detection and recognition market is growing rapidly, with the education sector being one of the fastest-growing nodes. The document highlights the potential of the Real-Time Engagement Tracker to transform presentation delivery from an art based on intuition into a data-driven practice, allowing presenters to connect more effectively with their audience.\n\nFurthermore, the document outlines the future plans and goals of the project, including the exploration of emerging technologies and startups that could redefine engagement tracking and provide a competitive advantage. The developers aim to stay flexible and open to unexpected trends or paradigm shifts, positioning the company to lead future developments in the field.\n\nOverall, the document provides a detailed summary of the Real-Time Engagement Tracker, its development, and its potential applications, highlighting the importance of understanding audience engagement and the need for a data-driven approach to presentation delivery."}, {"role": "user", "content": "who are the team members??"}, {"role": "assistant", "content": "The team members are Carlos Fernandez Olortegui, Alina Insam, Sumedh Bamane, and Charles Frelet. They hold the following roles: \n- Carlos Fernandez Olortegui: Principal Engineer and Director of Engineering at Salesforce\n- Alina Insam: Data Scientist and Customer Usage Insights Analyst at SAP\n- Sumedh Bamane: Principal Engineer and MBDS Student and SAP CT & Exp at Boehringer Ingelheim\n- Charles Frelet: Developer of the app and Data Engineer at Amaris, and MBDS Student."}, {"role": "user", "content": "how many models did they train and why did they choose the one they went ahead with?"}, {"role": "assistant", "content": "They trained 4 models: MobileNet, ResNet50, AlexNet, and ResNet18. They also mentioned using a Visual Transformer model, but the details of that are not provided in this section.\n\nThe choice of model was based on performance and generalization. Initially, MobileNet was used, but it had a modest accuracy of around 26%. ResNet50 showed more stable training behavior but had lower accuracy. AlexNet showed promising results but suffered from overfitting. ResNet18 did not perform well on the dataset.\n\nThe final choice of model is not explicitly stated in this section, but it is mentioned that the Visual Transformer was the solution, implying that it was the model that was ultimately chosen. However, the details of the Visual Transformer model are not provided in this section. \n\nIt can be inferred that the choice of model was based on a combination of factors, including accuracy, generalization, and stability, as well as the ability to improve performance through fine-tuning and adjustments to the dataset and training process."}, {"role": "user", "content": "why did the accuracy mattered to them?"}, {"role": "assistant", "content": "The accuracy mattered because the team's goal was to develop a reliable engagement detection system, and high accuracy was essential to ensure that the system could correctly interpret the audience's engagement levels. Inaccurate results would lead to misleading feedback for presenters, which would undermine the system's purpose. Additionally, the team aimed to commercialize their product, and high accuracy was crucial to attract and retain customers, ultimately impacting their revenue projections."}, {"role": "user", "content": "what was the problem in the first model?"}, {"role": "assistant", "content": "The problem with the first model, which was a custom lightweight CNN architecture inspired by MobileNet, was that it struggled with fine-grained emotion classification, likely due to inherent noise, facial ambiguity, and class overlap in the dataset. Despite careful training, the absolute accuracy was around 26%, indicating that the model had difficulty in accurately classifying emotions. Additionally, the interpretation of existing emotion labels into engagement categories was not always reliable, as it was highly context-dependent."}]